# -*- coding: utf-8 -*-
"""idxpartner_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dEiTKmS3mShFKEJOx9tAzAPNqrq-RJxA

# 1. Import Library
"""

import pandas as pd
import numpy as np

df = pd.read_csv('/content/drive/MyDrive/Dataset/loan_data_2007_2014.csv')

df

df.shape

df.info()

df.term

"""# 2. Data Cleaning"""

df.term = df.term.str.strip(' months')

df.term.value_counts()

"""Menghilangkan kolom yang tidak ada datanya sama sekali"""

df = df.drop(['id', 'member_id', 'Unnamed: 0', 'open_acc_6m', 'open_il_6m', 'open_il_12m', 'open_il_24m',
             'mths_since_rcnt_il', 'total_bal_il', 'il_util', 'open_rv_12m', 'open_rv_24m', 'max_bal_bc',
             'all_util', 'annual_inc_joint', 'dti_joint', 'verification_status_joint', 'inq_fi', 'total_cu_tl',
             'inq_last_12m'], axis = 1)

"""
Menghilangkan kolom yang tidak perlu"""

df.info()

df['loan_status'].unique()

df.loan_status.value_counts()

df = df[df["loan_status"].str.contains('Does not meet the credit policy. Status:Fully Paid') == False]
df = df[df["loan_status"].str.contains('Does not meet the credit policy. Status:Charged Off') == False]

df.shape

df.loan_status.value_counts()

"""# 3. Target Variable"""

df['good_bad'] = np.where(df.loc[:, 'loan_status'].isin(['Charged Off', 'Default', 'Late (31-120 days)',
                                                         'In Grace Period', 'Late (16-30 days)']), 1, 0)
df['good_bad']

df.info()

"""# 4. Missing Value"""

missing_values = pd.DataFrame(df.isnull().sum()/df.shape[0])

df.isnull().sum()

missing_values.sort_values(0, ascending = False)

missing_values = pd.DataFrame(df.isnull().sum()/df.shape[0])
missing_values_filter = missing_values[missing_values[0] > 0.40]

missing_values_filter.sort_values(0, ascending = False)

df.drop(['mths_since_last_record', 'mths_since_last_major_derog', 'desc', 'mths_since_last_delinq', 'next_pymnt_d'], axis = 1, inplace = True)

missing_values = pd.DataFrame(df.isnull().sum()/df.shape[0])
missing_values_filter = missing_values[missing_values[0] > 0.40]
missing_values_filter.sort_values(0, ascending = False)

loan_data = df

loan_data.select_dtypes(include = ['object', 'bool'])

loan_data.select_dtypes(include = ['object', 'bool']).columns

for col in loan_data.select_dtypes(include = ['object', 'bool']).columns:
  print(col)
  print(loan_data[col].unique())
  print(loan_data[col].dtype)
  print()

loan_data.drop(['emp_title', 'title', 'zip_code', 'application_type', 'url'], axis = 1, inplace = True)

loan_data.term = pd.to_numeric(loan_data.term)

loan_data.emp_length.unique()

loan_data['emp_length'] = loan_data['emp_length'].str.replace('\+ years', '')
loan_data['emp_length'] = loan_data['emp_length'].str.replace(' years', '')
loan_data['emp_length'] = loan_data['emp_length'].str.replace('< 1 year', str(0))
loan_data['emp_length'] = loan_data['emp_length'].str.replace(' year', '')

loan_data['emp_length'].fillna(value = 0, inplace = True)
loan_data['emp_length'] = pd.to_numeric(loan_data['emp_length'])

loan_data.emp_length.unique()

loan_data.info()

for col in loan_data.select_dtypes(include = ['int', 'float']).columns:
  print(col)
  print(loan_data[col].unique())
  print(loan_data[col].dtype)
  print()

loan_data.drop(['funded_amnt_inv', 'dti', 'delinq_2yrs', 'inq_last_6mths', 'policy_code','collections_12_mths_ex_med', 'acc_now_delinq'], axis = 1, inplace = True)

for col in loan_data.select_dtypes(include = ['object', 'bool']).columns:
  print(col)
  print(loan_data[col].unique())
  print(loan_data[col].dtype)
  print()

date_col = ['issue_d', 'earliest_cr_line', 'last_pymnt_d', 'last_credit_pull_d']

for col in date_col:
  loan_data[col] = loan_data[col].str.replace('-0', '-200')
  loan_data[col] = loan_data[col].str.replace('-1', '-201')
  loan_data[col] = loan_data[col].str.replace('-9', '-199')
  loan_data[col] = loan_data[col].str.replace('-8', '-198')
  loan_data[col] = loan_data[col].str.replace('-7', '-197')
  loan_data[col] = loan_data[col].str.replace('-6', '-197')
  loan_data[col] = pd.to_datetime(loan_data[col])

loan_data[date_col]

loan_data.drop('loan_status', axis = 1, inplace = True)

loan_data.info()

loan_data_dum = pd.get_dummies(loan_data)
loan_data_dum.shape

loan_data_dum.head()

from datetime import date

def date_columns(df, column):
  today_date = pd.to_datetime(date.today().strftime('%Y-%m-%d'))
  df[column] = pd.to_datetime(df[column], format = '%b-%y')
  df['mths_since_'+column] = round(pd.to_numeric((today_date - df[column]) / np.timedelta64(1, 'M')))
  df.drop(columns = [column], inplace = True)

for col in date_col:
  date_columns(loan_data_dum, col)

ldd_missing = pd.DataFrame(loan_data_dum.isnull().sum())

ldd_missing = ldd_missing[ldd_missing[0] > 0]

ldd_missing

missing_col = ['revol_util', 'tot_coll_amt', 'tot_cur_bal', 'total_rev_hi_lim', 'mths_since_last_pymnt_d', 'mths_since_last_credit_pull_d']

for col in missing_col:
  loan_data_dum[col].fillna(loan_data_dum[col].median(), inplace = True)

ldd_missing = pd.DataFrame(loan_data_dum.isnull().sum())
ldd_missing = ldd_missing[ldd_missing[0] > 0]
ldd_missing

for col in loan_data_dum.select_dtypes(include = ['int', 'float']).columns:
  print(col)
  print(loan_data_dum[col].unique())
  print(loan_data_dum[col].dtype)
  print()

column_minmax = loan_data_dum.select_dtypes(include = ['int', 'float']).columns
column_minmax

column_minmax_list = list(column_minmax)
column_minmax_list

column_minmax_list.remove('term')
column_minmax_list.remove('good_bad')
column_minmax_list

"""# 5. Data Splitting"""

from sklearn.compose import make_column_transformer
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

ct = make_column_transformer(
    (MinMaxScaler(), column_minmax_list))

loan_data_dum.head()

from sklearn.model_selection import train_test_split

X = loan_data_dum.drop('good_bad', axis = 1)
y = loan_data_dum['good_bad']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)

ct.fit(X_train)

X_train = ct.transform(X_train)
X_test = ct.transform(X_test)

y_train.value_counts(normalize = True)

y_test.value_counts(normalize = True),

X_train.shape, y_train.shape, X_test.shape, y_test.shape

"""# 6. Modelling"""

from sklearn.linear_model import LogisticRegression

model_1 = LogisticRegression()

model_1.fit(X_train, y_train)

y_pred = model_1.predict(X_test)

result = pd.DataFrame(list(zip(y_pred, y_test)), columns = ['y_pred', 'y_test'])

result.head()

from sklearn.metrics import accuracy_score
accuracy_score(y_test, y_pred)

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

cm = confusion_matrix(y_test, y_pred)

sns.heatmap(cm, annot = True, fmt='.0f', cmap = plt.cm.Blues)
plt.xlabel('y_pred')
plt.ylabel('y_test')
plt.show()

model_1.predict(X_test)

model_1.predict_proba(X_test)

y_pred = model_1.predict_proba(X_test)[:, 1]

y_pred > 5.0

(y_pred > 5.0).astype(int)

plt.hist(y_pred);

from sklearn.metrics import roc_curve

fpr, tpr, thresholds = roc_curve(y_test, y_pred)

j = tpr - fpr

ix = np.argmax(j)

best_thresh = thresholds[ix]
best_thresh

y_pred = model_1.predict_proba(X_test)[:, 1]
y_pred = (y_pred > best_thresh).astype(int)

cm = confusion_matrix(y_test, y_pred)

sns.heatmap(cm, annot = True, fmt='.0f', cmap = plt.cm.Blues)
plt.xlabel('y_pred')
plt.ylabel('y_test')
plt.show()

accuracy_score(y_test, y_pred)

model_1.coef_

X_train.shape, len(model_1.coef_[0])

model_1.intercept_